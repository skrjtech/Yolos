{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a980640-0c5d-4852-8d1c-2276026fa9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import yolos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458d1930-044d-47cb-a723-42b8f552cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOMODEL(torch.nn.Module):\n",
    "    def __init__(self, S: int = 7, B: int = 2, C: int = 20, *args, **kwargs) -> None:\n",
    "        super(YOLOMODEL, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.S, self.B, self.C = S, B, C\n",
    "        self.N = B * 5 + C\n",
    "\n",
    "class YoloLossModel(YOLOMODEL):\n",
    "    def __init__(self, lambdaobj: float = 5., lambdanoobj: float = .5, *args, **kwargs):\n",
    "        super(YoloLossModel, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.lambdaobj = lambdaobj\n",
    "        self.lambdanoobj = lambdanoobj\n",
    "\n",
    "        self.CI = [4, 9]  # Confidence Index\n",
    "        self.BI = [[0, 1, 2, 3], [5, 6, 7, 8]]  # BBoxIndex\n",
    "        self.LI = [self.B * 5 + idx for idx in range(self.C)]  # Label Index\n",
    "        self.XYI = [0, 1]  # XY or XYMin\n",
    "        self.WHI = [2, 3]  # WH or XYMax\n",
    "\n",
    "    def forward(self, P: torch.Tensor, T: torch.Tensor):\n",
    "        B, C, N = self.B, self.C, self.N\n",
    "        CI = self.CI\n",
    "        BI = self.BI\n",
    "        LI = self.LI\n",
    "        XYI = self.XYI\n",
    "        WHI = self.WHI\n",
    "\n",
    "        Batch = P.size(0)\n",
    "\n",
    "        coordMask = (T[..., 4] == 1).unsqueeze(-1).expand_as(T)\n",
    "        noobjMask = (T[..., 4] == 0).unsqueeze(-1).expand_as(T)\n",
    "\n",
    "        coordP = P[coordMask].reshape(-1, N)  # [coord_n, N]\n",
    "        noobjP = P[noobjMask].reshape(-1, N)  # [coord_n, N]\n",
    "\n",
    "        coordT = T[coordMask].reshape(-1, N)  # [coord_n, N]\n",
    "        noobjT = T[noobjMask].reshape(-1, N)  # [coord_n, N]\n",
    "\n",
    "        # Class Label\n",
    "        ClassP = coordP[..., LI].reshape(-1, C)  # [coord_n, C]\n",
    "        ClassT = coordT[..., LI].reshape(-1, C)  # [coord_n, C]\n",
    "        # No Object Confidence\n",
    "        NoObjP = noobjP[..., CI].reshape(-1, B)  # [nooobj_n, B]\n",
    "        NoObjT = noobjT[..., CI].reshape(-1, B)  # [nooobj_n, B]\n",
    "        # Object Confidence\n",
    "        ConfP = coordP[..., CI].reshape(-1, B);  # [coord_n, B]\n",
    "        # BBox\n",
    "        BBoxP = coordP[..., BI].reshape(-1, B, 4)  # [coord_n, B, 4(XYXY)]\n",
    "        BBoxT = coordT[..., BI].reshape(-1, B, 4)  # [coord_n, B, 4(XYXY)]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            iou, iouIndex = self.IoUCul(BBoxP.reshape(-1, 4), BBoxT.reshape(-1, 4))\n",
    "\n",
    "        Range = torch.arange(iouIndex.size(0)).long()\n",
    "        BBoxP = BBoxP[Range, iouIndex].reshape(-1, 4)\n",
    "        BBoxT = BBoxT[Range, iouIndex].reshape(-1, 4)\n",
    "        ConfP = ConfP[Range, iouIndex]\n",
    "\n",
    "        lossXY = torch.nn.functional.mse_loss(BBoxP[..., XYI], BBoxT[..., XYI], reduction=\"sum\")\n",
    "        lossWH = torch.nn.functional.mse_loss(torch.sqrt(BBoxP[..., WHI]), torch.sqrt(BBoxT[..., WHI]), reduction=\"sum\")\n",
    "        lossObj = torch.nn.functional.mse_loss(ConfP, iou, reduction=\"sum\")\n",
    "        lossNObj = torch.nn.functional.mse_loss(NoObjP, NoObjT, reduction=\"sum\")\n",
    "        lossClass = torch.nn.functional.mse_loss(ClassP, ClassT, reduction=\"sum\")\n",
    "        loss = (self.lambdaobj * (lossXY + lossWH) + self.lambdanoobj * (lossNObj) + (lossObj + lossClass)) / Batch\n",
    "        return loss\n",
    "\n",
    "    def IoUCul(self, P, T):\n",
    "        \"\"\"\n",
    "        P (input): [Batch, coord_n, xywh]\n",
    "        T (input): [Batch, coord_n, xywh]\n",
    "        \"\"\"\n",
    "\n",
    "        XYI = self.XYI\n",
    "        WHI = self.WHI\n",
    "\n",
    "        S = 7\n",
    "        P = P.clone()\n",
    "        T = T.clone()\n",
    "\n",
    "        PXYMIN = P[..., XYI] / float(S) - 0.5 * P[..., WHI]\n",
    "        PXYMAX = P[..., XYI] / float(S) + 0.5 * P[..., WHI]\n",
    "\n",
    "        TXYMIN = T[..., XYI] / float(S) - 0.5 * T[..., WHI]\n",
    "        TXYMAX = T[..., XYI] / float(S) + 0.5 * T[..., WHI]\n",
    "\n",
    "        lt = torch.max(PXYMIN, TXYMIN)\n",
    "        rb = torch.min(PXYMAX, TXYMAX)\n",
    "\n",
    "        wh = torch.clamp(rb - lt, min=0.)\n",
    "        intersect = (wh[..., 0] * wh[..., 1])\n",
    "\n",
    "        Area1 = (PXYMAX - PXYMIN)\n",
    "        Area1 = Area1[..., 0] * Area1[..., 1]\n",
    "        Area2 = (TXYMAX - TXYMIN)\n",
    "        Area2 = Area2[..., 0] * Area2[..., 1]\n",
    "        Union = Area1 + Area2 - intersect\n",
    "\n",
    "        iou = intersect / Union\n",
    "        return torch.max(iou.reshape(-1, 2), dim=1)\n",
    "\n",
    "class YoloV1(YOLOMODEL):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super(YoloV1, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.vgg = vgg = torchvision.models.vgg16(pretrained=True)\n",
    "        vgg.features.requires_grad_()\n",
    "        vgg.avgpool.requires_grad_()\n",
    "\n",
    "        vgg.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(512 * 7 * 7, 512),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(512, self.S * self.S * self.N),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return self.vgg(inp).reshape(-1, self.S, self.S, self.N)\n",
    "\n",
    "    def Testing(self):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(torch.rand(1, 3, 224, 224))\n",
    "\n",
    "def TorchIsistance(Tensor: any) -> torch.Tensor:\n",
    "\n",
    "    T = lambda: torch.Tensor(Tensor)\n",
    "    if isinstance(Tensor, list): return T()\n",
    "    elif isinstance(Tensor, tuple): return T()\n",
    "    elif isinstance(Tensor, np.ndarray): return T()\n",
    "    return Tensor\n",
    "\n",
    "def EncoderBBox(BBox: torch.Tensor, Width: int, Height: int, S: int = 7) -> torch.Tensor:\n",
    "\n",
    "    \"\"\"\n",
    "        BBox: [[Xmin, Ymin, Xmax, Ymax, Label],...]\n",
    "        Width:\n",
    "        Height:\n",
    "        S:\n",
    "        return [[XIndex, YIndex, CenterX, CenterY, Width, Height],...]\n",
    "    \"\"\"\n",
    "\n",
    "    BBox = TorchIsistance(BBox)\n",
    "    if BBox.dim() == 1: BBox = BBox.reshape(1, -1)\n",
    "\n",
    "    S = float(S)\n",
    "    Label = BBox[..., -1].unsqueeze(-1)\n",
    "    WH = torch.Tensor([Width, Height]).unsqueeze(0)\n",
    "\n",
    "    XYXY = BBox[..., :4] / torch.cat((WH, WH), dim=1)\n",
    "    XYC = (XYXY[..., [2, 3]] + XYXY[..., [0, 1]]) / 2.\n",
    "    WH = (XYXY[..., [2, 3]] - XYXY[..., [0, 1]])\n",
    "\n",
    "    XYI = (XYC * S).ceil() - 1.\n",
    "    XYN = (XYC - (XYI / S)) * S\n",
    "\n",
    "    return torch.cat((XYI, XYN, WH, Label), dim=1)\n",
    "\n",
    "def MakeTargetBBox(BBox: torch.Tensor, S: int, B: int, C: int) -> torch.Tensor:\n",
    "\n",
    "    \"\"\"\n",
    "        BBox: [[XIndex, YIndex, CenterX, CenterY, Width, Height, Label],...]\n",
    "        S:\n",
    "        B:\n",
    "        C\n",
    "        return Tensor(7, 7, B * 5 + C)\n",
    "    \"\"\"\n",
    "\n",
    "    BBox = TorchIsistance(BBox)\n",
    "    if (BBox.dim() == 1): BBox = BBox.reshape(1, -1)\n",
    "\n",
    "    N = B * 5 + C\n",
    "    Label = BBox[..., -1].unsqueeze(-1).long()\n",
    "    Target = torch.zeros(S, S, N)\n",
    "\n",
    "    X = BBox[..., 0].unsqueeze(-1).long()\n",
    "    Y = BBox[..., 1].unsqueeze(-1).long()\n",
    "\n",
    "    XYWH = BBox[..., [2, 3, 4, 5]]\n",
    "    Target[Y, X, [0, 1, 2, 3, 5, 6, 7, 8]] = torch.cat((XYWH, XYWH), dim=1)\n",
    "    Target[Y, X, [4, 9]] = torch.Tensor([1., 1.])\n",
    "    Target[Y, X, B * 5 + Label] = torch.Tensor([1.])\n",
    "\n",
    "    return Target\n",
    "\n",
    "def makeBBoxes(num: int=1):\n",
    "    BBoxes = []\n",
    "    while len(BBoxes) < num:\n",
    "        x1 = torch.randint(0, 350, size=(1,))\n",
    "        y1 = torch.randint(0, 350, size=(1,))\n",
    "        x2 = torch.randint(50, 400, size=(1,))\n",
    "        y2 = torch.randint(50, 400, size=(1,))\n",
    "        label = torch.randint(0, 3, size=(1,))\n",
    "        if x1 < x2 and y1 < y2:\n",
    "            BBoxes += [[x1, y1, x2, y2, label]]\n",
    "    return torch.Tensor(BBoxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a896bf-1011-48d9-bf3e-1f2012169af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "BBoxes = makeBBoxes()\n",
    "enc = EncoderBBox(BBoxes, 400, 400)\n",
    "Target = MakeTargetBBox(enc, 7, 2, 3).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fc1dd929-1a06-4885-99f3-1c8387170f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boxes = yolos.YoloBoxes.YoloBoxes(400, 400, C=3)\n",
    "for box in BBoxes:\n",
    "    x1, y1, x2, y2, id = list(map(lambda x: int(x.item()), box))\n",
    "    Boxes += yolos.YoloBoxes.YoloBox(\"UnKnow\", id, x1, y1, x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a26db1fe-5e6d-4850-83e5-f196f153143c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Width( 400 ), Height( 400 )\n",
       "  ( 0 )( objname: UnKnow | objid:     1 ), ( xmin:  192 | ymin:   86 ), ( xmax:  207 | ymax:  349 )\n",
       "  ( 1 )( objname: UnKnow | objid:     1 ), ( xmin:   61 | ymin:   21 ), ( xmax:  116 | ymax:  271 )\n",
       "  ( 2 )( objname: UnKnow | objid:     1 ), ( xmin:  150 | ymin:   66 ), ( xmax:  155 | ymax:  147 )"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c5cc0118-0e55-4124-886f-7ba299b97bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 7, 13])\n"
     ]
    }
   ],
   "source": [
    "Target = Boxes()\n",
    "print(Target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8a3922f9-0973-4a98-bbb4-be40f862210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossmodel = YoloLossModel(C=3)\n",
    "# lossmodel = yolos.Models.YoloLossModel(C=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4864b1f2-e2b2-4b8b-bf3c-376e0b9cbda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = torch.zeros(*(1, 3, 224, 224))\n",
    "Input = torch.FloatTensor(Data)\n",
    "Target = torch.FloatTensor(Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dbc4d104-58a6-4b20-8bf7-2a4434842bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/aios/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/aios/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "net = YoloV1(C=3)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "038a5a5f-085e-441f-87d2-9ef3b4c0c4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0000.00060\r"
     ]
    }
   ],
   "source": [
    "loss = 0.\n",
    "while not ((loss > 0.) and (loss < 0.001)):\n",
    "    optim.zero_grad()\n",
    "    pred = net(Data)\n",
    "    loss = lossmodel(pred, Target.unsqueeze(0))\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(f\"loss: {loss.item(): 011.05f}\", end=\"\\r\")\n",
    "pred = pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "35cb8a34-2f26-414c-a493-e01fcd4de194",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolos.YoloBoxes.YoloRoot(C=3)\n",
    "detect = yolos.YoloBoxes.Detect(400, 400)\n",
    "P, T = detect(pred.detach(), Target.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "66567644-2d9d-4ec1-b73b-dd7d901ff9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Width( 400 ), Height( 400 )\n",
       "  ( 0 )( objname: None | objid:     1 ), ( xmin: 0061.000 | ymin: 0022.000 ), ( xmax: 0116.000 | ymax: 0269.000 ), ( Confidence: 0.977 )\n",
       "  ( 1 )( objname: None | objid:     1 ), ( xmin: 0150.000 | ymin: 0065.000 ), ( xmax: 0155.000 | ymax: 0147.000 ), ( Confidence: 0.968 )\n",
       "  ( 2 )( objname: None | objid:     1 ), ( xmin: 0191.000 | ymin: 0085.000 ), ( xmax: 0206.000 | ymax: 0349.000 ), ( Confidence: 0.961 )\n",
       "  ( 3 )( objname: None | objid:     1 ), ( xmin: 0063.000 | ymin: -069.000 ), ( xmax: 0064.000 | ymax: 0330.000 ), ( Confidence: 0.825 )"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.ToInt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a51fb3fe-0800-48ac-a354-4c84224e37db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Width( 400 ), Height( 400 )\n",
       "  ( 0 )( objname: None | objid:     1 ), ( xmin: 0192.000 | ymin: 0085.000 ), ( xmax: 0207.000 | ymax: 0349.000 ), ( Confidence: 1.000 )\n",
       "  ( 1 )( objname: None | objid:     1 ), ( xmin: 0150.000 | ymin: 0066.000 ), ( xmax: 0155.000 | ymax: 0146.000 ), ( Confidence: 1.000 )\n",
       "  ( 2 )( objname: None | objid:     1 ), ( xmin: 0060.000 | ymin: 0021.000 ), ( xmax: 0116.000 | ymax: 0271.000 ), ( Confidence: 1.000 )"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.ToInt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "708aaaa2-8f1a-4253-ac48-f27bf47468c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TBox = [box()[2:] for box in Boxes.Decoder() for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b0955a15-56a0-4d4e-954f-69490e6fe635",
   "metadata": {},
   "outputs": [],
   "source": [
    "TBox = torch.Tensor(TBox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3f702841-c0a3-4f77-a5a9-15537aebef6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1., 1., 1.]),\n",
       "indices=tensor([0, 0, 0]))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IoUCul(TBox, TBox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "a68460a4-c6ec-4c8f-ba00-59c05e396578",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "660b1c6a-6108-43d3-92c2-41bb1b7f0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(box1, box2):\n",
    "\n",
    "    N, M = box1.size(0), box2.size(0)\n",
    "\n",
    "    PXYMIN = box1[..., [0, 1]].unsqueeze(1).expand(N, M, 2)\n",
    "    PXYMAX = box1[..., [2, 3]].unsqueeze(1).expand(N, M, 2)\n",
    "    TXYMIN = box2[..., [0, 1]].unsqueeze(0).expand(N, M, 2)\n",
    "    TXYMAX = box2[..., [2, 3]].unsqueeze(0).expand(N, M, 2)\n",
    "\n",
    "    MIN = torch.max(PXYMIN, TXYMIN)\n",
    "    MAX = torch.min(PXYMAX, TXYMAX)\n",
    "\n",
    "    Inter = torch.clamp(MAX - MIN, min=0)\n",
    "    Inter = Inter[..., 0] * Inter[..., 1]\n",
    "\n",
    "    AreaA = ((PXYMAX[..., 0] - PXYMIN[..., 0]) * (PXYMAX[..., 1] - PXYMIN[..., 1]))\n",
    "    AreaB = ((TXYMAX[..., 0] - TXYMIN[..., 0]) * (TXYMAX[..., 1] - TXYMIN[..., 1]))\n",
    "    Union = AreaA + AreaB - Inter\n",
    "    \n",
    "    return Inter / Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61c396f-8184-4ea2-b9bb-b2db2ad9afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBoxes1 = makeBBoxes(6)\n",
    "Boxes1 = yolos.YoloBoxes.YoloBoxes(400, 400, C=1)\n",
    "for box in BBoxes1:\n",
    "    x1, y1, x2, y2, id = list(map(lambda x: int(x.item()), box))\n",
    "    Boxes1 += yolos.YoloBoxes.YoloBox(\"UnKnow\", id, x1, y1, x2, y2)\n",
    "Boxes1 = torch.Tensor([box()[2:] for box in Boxes1.Decoder() for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9567cafc-2b3a-448c-b868-f4baf4a2fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBoxes2 = makeBBoxes(6)\n",
    "Boxes2 = yolos.YoloBoxes.YoloBoxes(400, 400, C=3)\n",
    "for box in BBoxes2:\n",
    "    x1, y1, x2, y2, id = list(map(lambda x: int(x.item()), box))\n",
    "    Boxes2 += yolos.YoloBoxes.YoloBox(\"UnKnow\", id, x1, y1, x2, y2)\n",
    "Boxes2 = torch.Tensor([box()[2:] for box in Boxes2.Decoder() for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d2a771b-d961-4ef9-b485-1381f493c8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 61.,  21., 116., 271.],\n",
       "        [ 61.,  21., 116., 271.],\n",
       "        [150.,  66., 155., 147.],\n",
       "        [150.,  66., 155., 147.],\n",
       "        [ 52., 171., 256., 190.],\n",
       "        [ 52., 171., 256., 190.],\n",
       "        [ 21., 112., 212., 358.],\n",
       "        [ 21., 112., 212., 358.],\n",
       "        [  3., 213., 354., 276.],\n",
       "        [  3., 213., 354., 276.],\n",
       "        [ 22., 143., 136., 200.],\n",
       "        [ 22., 143., 136., 200.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Boxes1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405ed692-8dfb-4c5c-904f-689d6e311d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[190., 295., 202., 379.],\n",
       "        [190., 295., 202., 379.],\n",
       "        [ 60., 104., 348., 131.],\n",
       "        [ 60., 104., 348., 131.],\n",
       "        [ 16.,  79., 307., 245.],\n",
       "        [ 16.,  79., 307., 245.],\n",
       "        [186.,  14., 364.,  52.],\n",
       "        [186.,  14., 364.,  52.],\n",
       "        [277., 148., 356., 275.],\n",
       "        [277., 148., 356., 275.],\n",
       "        [159.,  60., 223., 365.],\n",
       "        [159.,  60., 223., 365.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Boxes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8de0ab4-fffb-4590-a94b-c032b909a514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [0.0200, 0.0168],\n",
      "        [0.0934, 0.0802],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.1597, 0.1744],\n",
      "        [0.0000, 0.0000]])\n",
      "tensor([0.0000, 0.0200, 0.0934, 0.0000, 0.1744, 0.0000]) tensor([0, 0, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "P = Boxes1\n",
    "P[::2] = P[::2] + torch.randint(10, (4,))\n",
    "T = Boxes2[::2]\n",
    "N = T.size(0)\n",
    "print(N)\n",
    "iou = IoU(P, T).T.reshape(-1, 2)[::N + 1]\n",
    "score, index = torch.max(iou, dim=-1)\n",
    "print(iou)\n",
    "print(score, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fb133d63-8146-4062-9304-df71b0b536e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 48., 178., 359., 273.],\n",
       "        [ 48., 178., 359., 273.],\n",
       "        [241.,  33., 395., 159.],\n",
       "        [241.,  33., 395., 159.],\n",
       "        [314., 256., 315., 359.],\n",
       "        [314., 256., 315., 359.],\n",
       "        [ 88.,  33., 143.,  62.],\n",
       "        [ 88.,  33., 143.,  62.],\n",
       "        [ 86.,  75., 297., 247.],\n",
       "        [ 86.,  75., 297., 247.],\n",
       "        [138., 161., 254., 348.],\n",
       "        [138., 161., 254., 348.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Boxes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "33458649-4e95-4b45-91be-869349ac3b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 48., 178., 359., 273.],\n",
       "        [241.,  33., 395., 159.],\n",
       "        [314., 256., 315., 359.],\n",
       "        [ 88.,  33., 143.,  62.],\n",
       "        [ 86.,  75., 297., 247.],\n",
       "        [138., 161., 254., 348.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Boxes2.reshape(-1, 2, 4)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199626a-4718-40f4-9c66-c5c4493e305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def _Norm2DNorm(Boxes: torch.Tensor, S: int) -> torch.Tensor:\n",
    "    Target = torch.zeros_like(Boxes)\n",
    "    Target[..., [0, 1]] = Boxes[..., [0, 1]] / float(S) - .5 * Boxes[..., [2, 3]]\n",
    "    Target[..., [2, 3]] = Boxes[..., [0, 1]] / float(S) + .5 * Boxes[..., [2, 3]]\n",
    "    return Target\n",
    "\n",
    "def yololoss(S: int, B: int, C: int, LambdaObj: float=5., LambdaNoObj: float=.5) -> torch.Tensor:\n",
    "    N = B * 5 + C\n",
    "    CI = [4, 9] # Confidence\n",
    "    BI = [[0, 1, 2, 3], [5, 6, 7, 8]] # Boxes\n",
    "    LI = [B * 5 + idx for idx in range(C)] # Labels \n",
    "    XYI = [0, 1] # XYIndex\n",
    "    WHI = [2, 3] # WHIndex\n",
    "    def __CALL__(Prediction: torch.Tensor, Target: torch.Tensor):\n",
    "        BatchSize = Prediction.size(0)\n",
    "        coordMask = (Target[..., 4] == 1).unsqueeze(-1).expand_as(Target)\n",
    "        noobjMask = (Target[..., 4] == 0).unsqueeze(-1).expand_as(Target)\n",
    "\n",
    "        coordP = Prediction[coordMask].reshape(-1, N)  # [coord_n, N]\n",
    "        noobjP = Prediction[noobjMask].reshape(-1, N)  # [coord_n, N]\n",
    "\n",
    "        coordT = Target[coordMask].reshape(-1, N)  # [coord_n, N]\n",
    "        noobjT = Target[noobjMask].reshape(-1, N)  # [coord_n, N]\n",
    "\n",
    "        # Class Label\n",
    "        ClassP = coordP[..., LI].reshape(-1, C)  # [coord_n, C]\n",
    "        ClassT = coordT[..., LI].reshape(-1, C)  # [coord_n, C]\n",
    "        # No Object Confidence\n",
    "        NoObjP = noobjP[..., CI].reshape(-1, B)  # [nooobj_n, B]\n",
    "        NoObjT = noobjT[..., CI].reshape(-1, B)  # [nooobj_n, B]\n",
    "        # Object Confidence\n",
    "        ConfP = coordP[..., CI].reshape(-1, B);  # [coord_n, B]\n",
    "        # BBox\n",
    "        BBoxP = coordP[..., BI].reshape(-1, B, 4)  # [coord_n, B, 4(XYWH)]\n",
    "        BBoxT = coordT[..., BI].reshape(-1, B, 4)  # [coord_n, B, 4(XYWH)]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            BBoxP_ = BBoxP.clone()\n",
    "            BBoxT_ = BBoxT.clone()\n",
    "            \n",
    "            BBoxP_ = _Norm2DNorm(BBoxP_, S)\n",
    "            BBoxT_ = _Norm2DNorm(BBoxT_, S)\n",
    "\n",
    "            PXYMIN, PXYMAX = BBoxP_[..., [0, 1]], BBoxP_[..., [2, 3]]\n",
    "            TXYMIN, TXYMAX = BBoxT_[..., [0, 1]], BBoxT_[..., [2, 3]]\n",
    "            Min, Max = torch.max(PXYMIN, TXYMIN), torch.min(PXYMAX, TXYMAX)\n",
    "            WH = torch.clamp(Max - Min, min=0.)\n",
    "            Intersection = (WH[..., 0] * WH[..., 1])\n",
    "            Area1 = (PXYMAX - PXYMIN)\n",
    "            Area1 = Area1[..., 0] * Area1[..., 1]\n",
    "            Area2 = (TXYMAX - TXYMIN)\n",
    "            Area2 = Area2[..., 0] * Area2[..., 1]\n",
    "            Union = Area1 + Area2 - Intersection\n",
    "            iou = Intersection / Union\n",
    "            iou, iouIndex = torch.max(iou.reshape(-1, 2), dim=1)\n",
    "\n",
    "        Range = torch.arange(iouIndex.size(0)).long()\n",
    "        BBoxP = BBoxP[Range, iouIndex].reshape(-1, 4)\n",
    "        BBoxT = BBoxT[Range, iouIndex].reshape(-1, 4)\n",
    "        ConfP = ConfP[Range, iouIndex]\n",
    "        \n",
    "        lossXY = torch.nn.functional.mse_loss(BBoxP[..., XYI], BBoxT[..., XYI], reduction=\"sum\")\n",
    "        lossWH = torch.nn.functional.mse_loss(torch.sqrt(BBoxP[..., WHI]), torch.sqrt(BBoxT[..., WHI]), reduction=\"sum\")\n",
    "        lossObj = torch.nn.functional.mse_loss(ConfP, iou, reduction=\"sum\")\n",
    "        lossNObj = torch.nn.functional.mse_loss(NoObjP, NoObjT, reduction=\"sum\")\n",
    "        lossClass = torch.nn.functional.mse_loss(ClassP, ClassT, reduction=\"sum\")\n",
    "        loss = (LambdaObj * (lossXY + lossWH) + LambdaNoObj * (lossNObj) + (lossObj + lossClass)) / BatchSize\n",
    "        return loss\n",
    "    \n",
    "    return __CALL__\n",
    "\n",
    "from yolos.YoloBoxes import YoloRoot\n",
    "class YOLOMODEL(torch.nn.Module):\n",
    "    pass\n",
    "\n",
    "class YoloLossModel(YOLOMODEL, YoloRoot):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        YoloRoot.__init__(self, **kwargs)\n",
    "        \n",
    "    def forward(self, P: torch.Tensor, T: torch.Tensor):\n",
    "        return yololoss(self.S, self.B, self.C, self.LambdaObj, self.LambdaNoObj)(P, T)\n",
    "\n",
    "class YoloV1(YOLOMODEL, YoloRoot):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        YoloRoot.__init__(self, **kwargs)\n",
    "        \n",
    "        self.vgg = vgg = torchvision.models.vgg16(pretrained=True)\n",
    "        vgg.features.requires_grad_()\n",
    "        vgg.avgpool.requires_grad_()\n",
    "\n",
    "        vgg.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(512 * 7 * 7, 512),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(512, self.S * self.S * self.N),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return self.vgg(inp).reshape(-1, self.S, self.S, self.N)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    YoloRoot(C=3)\n",
    "    BBox1 = torch.zeros(1, 7, 7, 13)\n",
    "    BBox2 = torch.zeros(1, 7, 7, 13)\n",
    "    BBox1[0, 1, 1] = torch.Tensor([0.11, 0.11, 0.4, 0.4, .9, 0.9, 0.9, 0.4, 0.4, .9, 0., 1., 0.])\n",
    "    BBox2[0, 1, 1] = torch.Tensor([0.1, 0.1, 0.4, 0.4, 1., 0.1, 0.1, 0.4, 0.4, 1., 0., 0., 1.])\n",
    "    print(YoloLossModel()(BBox1, BBox2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
